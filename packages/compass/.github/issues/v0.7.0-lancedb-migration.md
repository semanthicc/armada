# v0.7.0: LanceDB Migration + Incremental Indexing

## Overview

Migrate vector embeddings from SQLite to LanceDB for:
- O(log n) ANN search instead of O(n) scan
- Native incremental updates (upsert)
- Better separation of concerns (heuristics vs vectors)

## Architecture Change

### Before (v0.6.0)
```
SQLite (semanthicc.db)
├── projects
├── memories (heuristics)
└── embeddings (vectors) ← PROBLEM: O(n) scan, no incremental
```

### After (v0.7.0)
```
SQLite (semanthicc.db)          LanceDB (~/.local/share/semanthicc/lance/)
├── projects                    └── {project_id}/
├── memories                        └── embeddings.lance
└── (no embeddings)
```

## Key LanceDB Features to Use

```typescript
// Connect to project-specific LanceDB
const db = await lancedb.connect(`~/.local/share/semanthicc/lance/${projectId}`);

// Create table with schema
const table = await db.createTable("embeddings", data);

// Incremental upsert (key: file_path + chunk_index)
await table
  .mergeInsert(["file_path", "chunk_index"])
  .whenMatchedUpdateAll()
  .whenNotMatchedInsertAll()
  .execute(newChunks);

// Delete stale chunks for a file
await table.delete(`file_path = '${filePath}'`);

// Vector search
const results = await table.vectorSearch(queryVector).limit(5).toArray();
```

## Tasks

### Phase 1: LanceDB Adapter
- [ ] Add `@lancedb/lancedb` dependency
- [ ] Create `src/lance/index.ts` - connection management
- [ ] Create `src/lance/embeddings.ts` - CRUD operations
- [ ] Handle per-project database paths

### Phase 2: Incremental Indexing
- [ ] Track file hashes in SQLite `projects` or new `file_hashes` table
- [ ] On index: compare current hash vs stored hash
- [ ] Only re-embed changed files
- [ ] Delete chunks for removed files

### Phase 3: Migration
- [ ] Update `indexProject()` to use LanceDB
- [ ] Update `searchCode()` to use LanceDB
- [ ] Remove `embeddings` table from SQLite schema
- [ ] Migration script for existing data (optional)

### Phase 4: Cleanup
- [ ] Remove SQLite embedding code
- [ ] Update tests
- [ ] Update documentation

## Schema Design

### SQLite: file_hashes (new table)
```sql
CREATE TABLE file_hashes (
  id INTEGER PRIMARY KEY,
  project_id INTEGER NOT NULL,
  file_path TEXT NOT NULL,
  file_hash TEXT NOT NULL,
  last_indexed_at INTEGER NOT NULL,
  FOREIGN KEY (project_id) REFERENCES projects(id),
  UNIQUE(project_id, file_path)
);
```

### LanceDB: embeddings
```typescript
interface EmbeddingRecord {
  file_path: string;      // "src/components/Button.tsx"
  chunk_index: number;    // 0, 1, 2...
  chunk_start: number;    // line number
  chunk_end: number;      // line number
  content: string;        // chunk text
  vector: number[];       // 384-dim embedding
}
```

## Incremental Indexing Flow

```
indexProject(projectPath)
  │
  ├─ walkProject() → list of files
  │
  ├─ For each file:
  │   ├─ computeHash(file)
  │   ├─ getStoredHash(projectId, filePath)
  │   │
  │   ├─ If hash unchanged → SKIP
  │   │
  │   ├─ If hash changed or new:
  │   │   ├─ Delete old chunks from LanceDB
  │   │   ├─ Split into chunks
  │   │   ├─ Embed chunks
  │   │   ├─ Upsert to LanceDB
  │   │   └─ Update hash in SQLite
  │   │
  │   └─ If file deleted:
  │       ├─ Delete chunks from LanceDB
  │       └─ Delete hash from SQLite
  │
  └─ Return stats
```

## Acceptance Criteria

- [ ] Vector search uses LanceDB (faster for large projects)
- [ ] Re-indexing only processes changed files
- [ ] Deleted files have their chunks removed
- [ ] All existing tests pass
- [ ] New tests for incremental behavior
- [ ] SQLite only contains heuristics/memories

## Performance Expectations

| Metric | Before (SQLite) | After (LanceDB) |
|--------|-----------------|-----------------|
| Initial index (1000 files) | ~60s | ~60s (same) |
| Re-index (10 changed files) | ~60s | ~6s |
| Vector search (10k chunks) | ~500ms | ~50ms |

## Dependencies

```json
{
  "@lancedb/lancedb": "^0.4.0"
}
```

## References

- LanceDB TypeScript docs: https://lancedb.github.io/lancedb/
- MergeInsert API: https://lancedb.github.io/lancedb/js/classes/Table/#mergeinsert
